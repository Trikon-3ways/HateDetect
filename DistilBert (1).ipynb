{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tTCGxPSYdnLi",
        "outputId": "e037f78a-3f01-47cd-80ab-7ca644e3294b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "0vfIYd6Hd6O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7EamKN-d8hU",
        "outputId": "ad855608-7cda-4fdd-f898-b4125f0aa034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('/content/drive/MyDrive/train.xlsx') # Use read_excel to read .xlsx files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_zP9RpReJU9",
        "outputId": "03c7a99e-bcf8-4272-b903-257c89fb7221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['comment_text'].tolist()  # Convert to list for processing\n",
        "y = df[df.columns[2:]].values"
      ],
      "metadata": {
        "id": "AC7-oLd3eQnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['comment_text'].tolist()  # Convert to list for processing\n",
        "y = df[df.columns[2:]].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L-AsiQ_eTJK",
        "outputId": "31d675ff-7bf7-4af3-a314-9eecc62acbf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(texts):\n",
        "    return tokenizer(texts, padding=True, truncation=True, return_tensors='tf')"
      ],
      "metadata": {
        "id": "b5PeABbSeVxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mhW5Otj9xGRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(texts):\n",
        "    # Check if each element in 'texts' is a string\n",
        "    valid_texts = [text for text in texts if isinstance(text, str)]\n",
        "    return tokenizer(valid_texts, padding=True, truncation=True, return_tensors='tf')"
      ],
      "metadata": {
        "id": "an1UgDcQeZLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "import tensorflow as tf\n",
        "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification\n",
        "\n",
        "# Assuming X and y are your original data\n",
        "# and preprocess_function is defined as in your provided code\n",
        "\n",
        "# Apply preprocessing, but keep track of the indices of valid texts\n",
        "valid_indices = []\n",
        "def preprocess_function(texts):\n",
        "    valid_texts = [text for text in texts if isinstance(text, str)]\n",
        "    valid_indices.extend([i for i, text in enumerate(texts) if isinstance(text, str)])  # Store indices of valid texts\n",
        "    return tokenizer(valid_texts, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "encoded_X = preprocess_function(X)\n",
        "\n",
        "# Filter y to match the valid texts\n",
        "y_filtered = y[valid_indices]\n",
        "\n",
        "# Now create the tf.data.Dataset\n",
        "tf_dataset = tf.data.Dataset.from_tensor_slices((dict(encoded_X), y_filtered))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgI_bc7NebKE",
        "outputId": "d4086036-11fd-43c0-acd3-f5f3605bb098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 0.7\n",
        "val_size = 0.2\n",
        "test_size = 0.1\n",
        "\n",
        "train_dataset = tf_dataset.take(int(len(X) * train_size))\n",
        "val_dataset = tf_dataset.skip(int(len(X) * train_size)).take(int(len(X) * val_size))\n",
        "test_dataset = tf_dataset.skip(int(len(X) * (train_size + val_size)))"
      ],
      "metadata": {
        "id": "7f7aQFLyecv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.batch(16).shuffle(1000).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(16).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(16).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "2UiiV4dNefHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),  # since logits are used\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "qt6HsPn8eiRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset, epochs=10, validation_data=val_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVkVFvu6ei5l",
        "outputId": "c3b203ab-2bf5-49b4-95b0-7167c247c4f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "447/447 [==============================] - 469s 955ms/step - loss: 0.1019 - accuracy: 0.9094 - val_loss: 0.0516 - val_accuracy: 0.9956\n",
            "Epoch 2/10\n",
            "447/447 [==============================] - 433s 968ms/step - loss: 0.0470 - accuracy: 0.9889 - val_loss: 0.0487 - val_accuracy: 0.9956\n",
            "Epoch 3/10\n",
            "447/447 [==============================] - 433s 968ms/step - loss: 0.0372 - accuracy: 0.9752 - val_loss: 0.0512 - val_accuracy: 0.9887\n",
            "Epoch 4/10\n",
            "447/447 [==============================] - 432s 966ms/step - loss: 0.0302 - accuracy: 0.9178 - val_loss: 0.0544 - val_accuracy: 0.9956\n",
            "Epoch 5/10\n",
            "447/447 [==============================] - 431s 965ms/step - loss: 0.0251 - accuracy: 0.8409 - val_loss: 0.0577 - val_accuracy: 0.9877\n",
            "Epoch 6/10\n",
            "447/447 [==============================] - 431s 965ms/step - loss: 0.0208 - accuracy: 0.9293 - val_loss: 0.0616 - val_accuracy: 0.7475\n",
            "Epoch 7/10\n",
            "447/447 [==============================] - 431s 965ms/step - loss: 0.0181 - accuracy: 0.5857 - val_loss: 0.0671 - val_accuracy: 0.9701\n",
            "Epoch 8/10\n",
            "447/447 [==============================] - 432s 967ms/step - loss: 0.0153 - accuracy: 0.7696 - val_loss: 0.0691 - val_accuracy: 0.9314\n",
            "Epoch 9/10\n",
            "447/447 [==============================] - 431s 965ms/step - loss: 0.0108 - accuracy: 0.7450 - val_loss: 0.0835 - val_accuracy: 0.6627\n",
            "Epoch 10/10\n",
            "447/447 [==============================] - 433s 969ms/step - loss: 0.0103 - accuracy: 0.4566 - val_loss: 0.0710 - val_accuracy: 0.8676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import Precision, Recall, CategoricalAccuracy\n",
        "\n",
        "pre = Precision()\n",
        "re = Recall()\n",
        "acc = CategoricalAccuracy()\n",
        "\n",
        "# Evaluate on the test set\n",
        "for batch in test_dataset:\n",
        "    X_true, y_true = batch\n",
        "    yhat = model.predict(X_true)\n",
        "\n",
        "    # Apply sigmoid and round predictions\n",
        "    yhat = tf.nn.sigmoid(yhat.logits)\n",
        "    yhat = tf.round(yhat)\n",
        "\n",
        "    # Update metrics\n",
        "    pre.update_state(y_true, yhat)\n",
        "    re.update_state(y_true, yhat)\n",
        "    acc.update_state(y_true, yhat)\n",
        "\n",
        "# Print results\n",
        "print(f'Precision: {pre.result().numpy()}, Recall: {re.result().numpy()}, Accuracy: {acc.result().numpy()}')"
      ],
      "metadata": {
        "id": "1Cc1rF8mem54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "587d8dcd-0bbd-41ae-9b5f-97e6d8894313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 311ms/step\n",
            "1/1 [==============================] - 0s 308ms/step\n",
            "1/1 [==============================] - 0s 352ms/step\n",
            "1/1 [==============================] - 0s 352ms/step\n",
            "1/1 [==============================] - 0s 331ms/step\n",
            "1/1 [==============================] - 0s 340ms/step\n",
            "1/1 [==============================] - 0s 323ms/step\n",
            "1/1 [==============================] - 0s 339ms/step\n",
            "1/1 [==============================] - 0s 327ms/step\n",
            "1/1 [==============================] - 0s 326ms/step\n",
            "1/1 [==============================] - 0s 345ms/step\n",
            "1/1 [==============================] - 0s 333ms/step\n",
            "1/1 [==============================] - 0s 369ms/step\n",
            "1/1 [==============================] - 0s 367ms/step\n",
            "1/1 [==============================] - 0s 378ms/step\n",
            "1/1 [==============================] - 0s 387ms/step\n",
            "1/1 [==============================] - 0s 419ms/step\n",
            "1/1 [==============================] - 0s 360ms/step\n",
            "1/1 [==============================] - 0s 344ms/step\n",
            "1/1 [==============================] - 0s 333ms/step\n",
            "1/1 [==============================] - 0s 324ms/step\n",
            "1/1 [==============================] - 0s 334ms/step\n",
            "1/1 [==============================] - 0s 325ms/step\n",
            "1/1 [==============================] - 0s 324ms/step\n",
            "1/1 [==============================] - 0s 324ms/step\n",
            "1/1 [==============================] - 0s 322ms/step\n",
            "1/1 [==============================] - 0s 323ms/step\n",
            "1/1 [==============================] - 0s 321ms/step\n",
            "1/1 [==============================] - 0s 347ms/step\n",
            "1/1 [==============================] - 0s 322ms/step\n",
            "1/1 [==============================] - 0s 317ms/step\n",
            "1/1 [==============================] - 0s 327ms/step\n",
            "1/1 [==============================] - 0s 336ms/step\n",
            "1/1 [==============================] - 0s 325ms/step\n",
            "1/1 [==============================] - 0s 323ms/step\n",
            "1/1 [==============================] - 0s 372ms/step\n",
            "1/1 [==============================] - 0s 323ms/step\n",
            "1/1 [==============================] - 0s 433ms/step\n",
            "1/1 [==============================] - 0s 380ms/step\n",
            "1/1 [==============================] - 0s 372ms/step\n",
            "1/1 [==============================] - 0s 350ms/step\n",
            "1/1 [==============================] - 0s 323ms/step\n",
            "1/1 [==============================] - 0s 299ms/step\n",
            "1/1 [==============================] - 0s 322ms/step\n",
            "1/1 [==============================] - 0s 314ms/step\n",
            "1/1 [==============================] - 0s 303ms/step\n",
            "1/1 [==============================] - 0s 305ms/step\n",
            "1/1 [==============================] - 0s 306ms/step\n",
            "1/1 [==============================] - 0s 305ms/step\n",
            "1/1 [==============================] - 0s 305ms/step\n",
            "1/1 [==============================] - 0s 303ms/step\n",
            "1/1 [==============================] - 0s 305ms/step\n",
            "1/1 [==============================] - 0s 307ms/step\n",
            "1/1 [==============================] - 0s 307ms/step\n",
            "1/1 [==============================] - 0s 304ms/step\n",
            "1/1 [==============================] - 0s 312ms/step\n",
            "1/1 [==============================] - 0s 304ms/step\n",
            "1/1 [==============================] - 0s 306ms/step\n",
            "1/1 [==============================] - 0s 302ms/step\n",
            "1/1 [==============================] - 0s 303ms/step\n",
            "1/1 [==============================] - 0s 302ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "Precision: 0.7124183177947998, Recall: 0.26913580298423767, Accuracy: 0.9164990186691284\n"
          ]
        }
      ]
    }
  ]
}